# sync.Pool을 제대로 이해하려면 GMP 모델부터 알아야 한다

Go로 고성능 네트워크 프로그램을 작성하다 보면 sync.Pool을 자주 마주치게 된다. 대부분의 글에서는 "객체를 재사용해서 GC 부담을 줄인다" 정도로 설명하고 넘어가는데, 실제로 내부에서 어떻게 동작하는지 궁금해서 파보기 시작했다.

블록체인 코어 개발자를 목표로 공부하면서 Geth 코드를 읽다가 sync.Pool이 곳곳에 쓰이는 걸 발견했다. 단순히 "메모리 재사용"이라고만 이해하고 넘어가기엔 뭔가 찜찜했다. 왜 이게 락 없이 안전한 건지, 어떻게 이렇게 빠른 건지 제대로 알고 싶었다.

결론부터 말하면, sync.Pool을 제대로 이해하려면 Go 런타임의 스케줄링 구조(GMP 모델)를 먼저 알아야 한다. 이 글에서는 그 과정을 정리해본다.

---

## Geth 코드에서 발견한 패턴

트랜잭션 풀 코드를 보다가 이런 패턴을 발견했다.

```go
// go-ethereum/core/txpool 쪽 코드
var txLookupPool = sync.Pool{
    New: func() interface{} {
        return new(txLookup)
    },
}

func (pool *LegacyPool) add(tx *types.Transaction) (replaced bool, err error) {
    lookup := txLookupPool.Get().(*txLookup)
    defer txLookupPool.Put(lookup)
    
    // ... 트랜잭션 처리 로직 ...
}
```

왜 그냥 `new(txLookup)`을 안 하고 굳이 Pool을 쓰는 걸까. Geth뿐 아니라 RLP 인코딩, EVM 스택 등 성능이 중요한 곳에서 계속 이 패턴이 보였다. 제대로 이해하고 넘어가야겠다고 생각했다.

---

## 전통적인 멀티스레드 모델의 한계

일반적인 멀티스레드 프로그래밍에서는 OS 스레드가 CPU 코어를 두고 경쟁한다.

```
스레드 T1, T2, T3, T4, T5 ...
        |
        v
  OS 스케줄러 (커널)
        |
        v
  CPU 0   CPU 1   CPU 2   CPU 3
```

문제는 스레드 간 전환(Context Switching)이 발생할 때마다 커널이 개입한다는 점이다. 레지스터 저장, 복원, TLB 플러시 등의 작업이 필요하고, 이 비용이 1~10 마이크로초 정도 든다. 스레드가 수백 개만 되어도 스위칭 오버헤드가 무시할 수 없는 수준이 된다.

그리고 스레드 하나당 스택 메모리가 보통 1~8MB 필요하다. 연결 10,000개를 동시에 처리하려고 스레드 10,000개를 만들면 그것만으로 수십 GB 메모리가 필요해진다.

블록체인 노드 관점에서 생각해보면 이게 더 심각하다. Ethereum 메인넷 기준으로 피어 연결이 50~200개, 블록당 트랜잭션이 수백에서 수천 개다. 각 연결마다, 각 트랜잭션마다 스레드를 만들면 금방 한계에 부딪힌다.

그리고 합의 알고리즘에서 타이밍이 중요하다. CometBFT 같은 BFT 합의에서 메시지 응답이 늦으면 정상 노드가 비잔틴으로 의심받을 수 있다. OS 스레드 스위칭이 많아지면 이런 타이밍 문제가 생길 수 있다.

---

## Go의 해결책: GMP 모델

Go는 이 문제를 해결하기 위해 자체 스케줄링 시스템을 도입했다. 흔히 GMP 모델이라고 부른다.

- G (Goroutine): 고루틴. 우리가 `go func()`으로 만드는 것
- M (Machine): OS 스레드. 실제로 CPU에서 실행되는 단위
- P (Processor): 논리적 프로세서. 고루틴을 M에 할당하는 중간 계층

```
                    Go 런타임 영역 (유저 공간)
+------------------------------------------------------------------+
|                                                                   |
|    G1  G2  G3  G4  G5  G6  ...  G10000  <- 고루틴 10,000개        |
|     |   |   |   |   |   |         |                              |
|     +---+---+---+---+---+---------+                              |
|                 |                                                 |
|                 v                                                 |
|    +---------------------------------------------+               |
|    |         Go 스케줄러 (유저 공간에서 동작)      |               |
|    +---------------------------------------------+               |
|                 |                                                 |
|        +--------+--------+--------+                              |
|        v        v        v        v                              |
|       P0       P1       P2       P3      <- GOMAXPROCS 개수      |
|       |        |        |        |                                |
+-------+--------+--------+--------+--------------------------------+
        |        |        |        |
        v        v        v        v
       M0       M1       M2       M3       <- OS 스레드
        |        |        |        |
========+========+========+========+====================================
        |        |        |        |         OS 커널 영역
        v        v        v        v
      CPU0     CPU1     CPU2     CPU3
```

핵심은 2단계 스케줄링이다.

1단계에서 고루틴들이 P를 두고 경쟁한다. 이 과정은 Go 런타임이 유저 공간에서 처리한다. 커널 개입이 없어서 빠르다.

2단계에서 M(OS 스레드)이 CPU를 두고 경쟁한다. 이건 OS 커널이 관리한다. 하지만 M의 개수가 P의 개수와 비슷하므로 커널 레벨 스위칭이 최소화된다.

비유하자면, 직원 10,000명이 사장(OS)한테 직접 보고하는 게 아니라, 팀장 4명(P)한테 먼저 배정되고, 팀장이 사장한테 보고하는 구조다. 대부분의 스케줄링 결정을 팀장 선에서 처리하니까 사장(커널)이 개입할 일이 줄어든다.

---

## 숫자로 보는 차이

| 항목 | OS 스레드 | 고루틴 |
|------|-----------|--------|
| 스택 메모리 | 1~8MB (고정) | 2KB (동적 증가) |
| 생성 비용 | ~1ms | ~0.3μs |
| 스위칭 비용 | 1~10μs (커널 개입) | 0.1~0.2μs (유저 공간) |
| 생성 가능 개수 | 수천 개 | 수십만 개 |

고루틴 10,000개를 만들어도 메모리는 약 20MB밖에 안 든다. OS 스레드로 했으면 최소 10GB다. 500배 차이.

고루틴이 2KB로 시작해서 어떻게 복잡한 작업을 처리하냐면, 스택이 동적으로 확장된다. Go 런타임이 스택 오버플로우를 감지하면 더 큰 스택을 할당하고 기존 내용을 복사한다. 사용자는 신경 쓸 필요 없다.

---

## P의 개수는 고정이다

여기서 중요한 점이 있다. P의 개수는 프로그램 시작 시 GOMAXPROCS 값으로 고정된다. 보통 CPU 코어 수와 같다. 8코어 CPU라면 P가 8개다.

고루틴을 아무리 많이 만들어도 P가 늘어나지 않는다.

```
CPU 8코어 -> GOMAXPROCS = 8 -> P 8개 (고정)

고루틴 10,000개 만들어도 P는 여전히 8개
동시에 실행되는 고루틴도 최대 8개
```

나머지 9,992개의 고루틴은 P의 로컬 런큐에서 대기하거나 글로벌 런큐에 들어간다. 실행 중인 고루틴이 I/O 대기 상태가 되면 잠깐 빠지고, 대기 중이던 다른 고루틴이 실행된다.

이게 바로 sync.Pool 이해의 핵심 열쇠다.

---

## 다시 sync.Pool로 돌아와서

이제 sync.Pool 이야기를 할 수 있다.

sync.Pool은 P마다 로컬 풀을 가지고 있다.

```
+-------------------------------------------------------------+
|                       sync.Pool                             |
+-------------------------------------------------------------+
|                                                             |
|   P0의 poolLocal      P1의 poolLocal      P2의 poolLocal    |
|  +-------------+     +-------------+     +-------------+    |
|  | private: *A |     | private: *D |     | private: nil|    |
|  +-------------+     +-------------+     +-------------+    |
|  | shared:     |     | shared:     |     | shared:     |    |
|  |  [*B, *C]   |     |  [*E]       |     |  []         |    |
|  +-------------+     +-------------+     +-------------+    |
|                                                             |
+-------------------------------------------------------------+
```

각 P의 풀에는 두 가지 저장소가 있다.

- private: 해당 P 전용. 딱 1개만 저장. 락 없이 접근.
- shared: 다른 P도 접근 가능. 여러 개 저장. 동기화 필요.

---

## Get() 호출 시 일어나는 일

pool.Get()을 호출하면 다음 순서로 객체를 찾는다.

```
pool.Get() 호출
      |
      v
+-------------------------+
| 현재 P의 private 확인    |
+-----------+-------------+
            |
            v
       있나? -----Yes----> 반환 (락 없음, 가장 빠름)
            |
           No
            |
            v
+-------------------------+
| 현재 P의 shared 확인     |
+-----------+-------------+
            |
            v
       있나? -----Yes----> 반환
            |
           No
            |
            v
+-------------------------+
| 다른 P들의 shared 순회   |  <- Work Stealing
| (훔쳐오기 시도)          |
+-----------+-------------+
            |
            v
       있나? -----Yes----> 반환
            |
           No
            |
            v
+-------------------------+
| New() 함수 호출          |
| 새 객체 생성             |
+-----------+-------------+
            |
            v
         반환
```

---

## 왜 private 접근에 락이 필요 없는가

처음에 이게 이해가 안 됐다. 고루틴이 10,000개인데 어떻게 락 없이 동시 접근을 막는다는 건가.

답은 간단했다. 동시에 접근할 일이 없다.

P는 한 번에 하나의 고루틴만 실행한다. P0에서 G1이 실행 중이면, G5, G100, G9999 같은 다른 고루틴들은 전부 대기 상태다. 대기 중인 고루틴은 코드 실행 자체를 못 한다. pool.Get()을 호출할 수가 없다.

```
시점 T에서의 상태:

P0: G1 실행 중      |  P1: G5 실행 중      |  P2: G100 실행 중
    |              |      |              |      |
    v              |      v              |      v
G2, G3, G4 대기    |  G6, G7 대기        |  G101, G102 대기
(런큐에서 대기중)   |  (런큐에서 대기중)   |  (런큐에서 대기중)
```

P0에서 G1이 pool.Get()을 호출해서 P0의 private에 접근할 때, G2, G3, G4는 코드 실행 자체가 불가능하다. 그들은 P0의 런큐에서 대기 상태다. pool.Get()을 호출할 수조차 없다.

그래서 private은 락이 필요 없다. 경쟁 상대가 없다.

회사에 냉장고가 4개 있다고 생각하면 된다. 팀장 4명이 각자 냉장고 하나를 전담한다. 팀원은 자기 팀장의 냉장고만 이용하는데, 한 번에 팀원 1명만 냉장고 앞에 설 수 있다. 나머지는 줄 서서 대기한다. 자물쇠가 필요 없다. 어차피 혼자니까.

---

## 풀이 P 개수만큼만 있어도 충분한 이유

고루틴이 10,000개여도 동시에 버퍼를 요청할 수 있는 건 P 개수만큼이다. 8코어면 최대 8개.

나머지 9,992개는 대기 중이라 버퍼를 요청할 일 자체가 없다. 그래서 풀이 8개면 고루틴 10,000개를 커버할 수 있다.

만약 고루틴마다 풀을 따로 가지고 있다면 대부분 대기 중인데 쓸데없이 메모리를 잡아먹고 있는 셈이 된다.

---

## Work Stealing

P0의 풀이 비어있는데 P2의 풀에는 버퍼가 남아있는 상황이 있을 수 있다. 이때 P0은 P2의 shared에서 버퍼를 훔쳐온다.

```go
// sync.Pool.getSlow() 내부 로직 (간략화)
func (p *Pool) getSlow() interface{} {
    // 모든 P의 shared 순회
    for i := 0; i < int(size); i++ {
        other := indexLocal(local, (pid+i+1)%int(size))
        if x, _ := other.shared.popTail(); x != nil {
            return x  // 다른 P에서 훔쳐옴
        }
    }
    return nil
}
```

이 방식으로 바쁜 P는 한가한 P에서 자원을 가져오고, 전체적으로 자원이 재분배된다. New() 호출을 줄이고, GC 부담도 줄어든다.

눈치 빠른 사람은 popHead와 popTail의 차이를 발견했을 것이다. 자기 P의 shared에서 꺼낼 때는 popHead를 쓰고, 다른 P의 shared에서 훔칠 때는 popTail을 쓴다. shared가 lock-free 큐로 구현되어 있어서 Head와 Tail을 분리하면 충돌 가능성이 최소화된다.

---

## RWMutex의 RLock과 다른 점

RLock도 읽기 락이라 여러 고루틴이 동시에 잡을 수 있다. 하지만 여전히 락을 잡는 행위 자체가 있다.

```go
// RWMutex.RLock 내부 (간략화)
func (rw *RWMutex) RLock() {
    // 여전히 atomic 연산이 필요
    if atomic.AddInt32(&rw.readerCount, 1) < 0 {
        // 쓰기 대기자가 있으면 블록
        runtime_SemacquireRWMutexR(...)
    }
}
```

atomic 연산으로 읽기 카운터를 증가시키고, 쓰기 대기자가 있는지 확인하는 로직이 들어간다. CPU 코어 간 캐시 동기화가 필요하다.

sync.Pool의 private 접근은 이런 게 아예 없다. 그냥 값을 가져온다.

```go
// 그냥 값 가져오기
x := l.private
l.private = nil
```

P가 한 번에 하나의 고루틴만 실행하니까 경쟁 자체가 발생하지 않는다.

| 방식 | 락 연산 | atomic 연산 | 캐시 동기화 |
|------|---------|-------------|-------------|
| Mutex.Lock | O | O | O |
| RWMutex.RLock | 가벼운 락 | O | O |
| sync.Pool private | X | X | X |

이 차이가 초당 수십만 번 반복되면 엄청난 성능 차이로 이어진다.

---

## 블록체인 코어에서의 활용

Geth 코드를 보면 곳곳에서 sync.Pool을 쓴다.

트랜잭션 처리에서:

```go
// core/types/transaction.go
var txPool = sync.Pool{
    New: func() interface{} {
        return new(Transaction)
    },
}
```

Ethereum 메인넷에서 블록당 트랜잭션이 수백 개다. 매번 new(Transaction) 하면 힙 할당이 발생하고, GC가 추적해야 할 객체가 늘어나고, GC Pause 시간이 늘어난다.

RLP 인코딩/디코딩에서:

```go
// rlp/encode.go
var encBufPool = sync.Pool{
    New: func() interface{} {
        return new(encBuffer)
    },
}
```

모든 블록체인 데이터는 RLP로 직렬화된다. 블록 하나에 수천 번의 인코딩이 필요할 수 있다.

EVM 실행에서:

```go
// core/vm/interpreter.go
var stackPool = sync.Pool{
    New: func() interface{} {
        return &Stack{data: make([]uint256.Int, 0, 16)}
    },
}
```

스마트 컨트랙트 실행마다 스택이 필요하다. 컨트랙트가 다른 컨트랙트를 호출하면 스택이 더 필요하다.

---

## CometBFT는 왜 Mutex를 많이 쓸까

CometBFT(구 Tendermint) 코드를 보면 sync.Pool보다 Mutex를 더 많이 쓴다.

```go
// CometBFT consensus/state.go
type State struct {
    mtx sync.RWMutex  // 합의 상태 보호
    // ...
}
```

합의 엔진에서는 정확성이 성능보다 중요하기 때문이다. 합의 상태가 잘못되면 체인 분기가 발생한다. Lock-Free 구조는 디버깅이 어렵고, 미묘한 레이스 컨디션이 네트워크 전체를 마비시킬 수 있다.

다만 네트워크 I/O 처리 같은 곳에서는 Channel과 Goroutine을 적극 활용한다.

```go
// p2p/conn/connection.go
type MConnection struct {
    sendQueue chan PacketMsg  // Channel 기반 통신
    // ...
}
```

| 영역 | 전략 | 이유 |
|------|------|------|
| 합의 상태 | Mutex | 정확성 최우선 |
| 네트워크 I/O | Channel + Goroutine | 처리량 최적화 |
| 임시 버퍼 | sync.Pool | 메모리 효율화 |

---

## GOGC 튜닝 vs Zero Allocation

블록체인 코어 개발에서 GC 최적화 이야기가 나오면 두 가지 접근이 있다.

GOGC 튜닝:

```bash
GOGC=200 ./geth  # 기본값 100, 높이면 GC 빈도 감소
```

코드 수정 없이 GC 빈도를 줄일 수 있지만, 메모리 사용량이 늘어나고 GC 발생 시 Pause 시간이 더 길어질 수 있다.

Zero Allocation:

```go
// 할당 발생
func processBlock(block *Block) *Result {
    result := new(Result)  // 힙 할당
    // ...
    return result
}

// Zero Allocation
func processBlock(block *Block, result *Result) {
    // result를 재사용, 할당 없음
    // ...
}
```

근본적으로 GC 부담을 제거하지만 코드가 복잡해진다.

현실적으로는 이렇게 접근하는 게 좋을 것 같다.

1. 먼저 `go tool pprof`로 할당 핫스팟 찾기
2. 핫스팟에 sync.Pool 적용 (가성비 좋음)
3. 그래도 부족하면 Zero Allocation (성능 크리티컬 경로만)
4. GOGC는 마지막 수단 (다른 최적화 다 해보고 나서)

---

## 주의사항

sync.Pool은 캐시일 뿐 영구 저장소가 아니다. GC가 발생하면 풀의 내용이 사라질 수 있다.

```go
pool.Put(buf)
runtime.GC()
buf = pool.Get() // New()가 다시 호출될 수 있음
```

블록 캐시 같은 걸 sync.Pool로 구현하면 안 된다. GC 후에 캐시가 비워지기 때문이다. 영구 캐시가 필요하면 LRU Cache 같은 걸 써야 한다.

그리고 풀에서 꺼낸 객체는 이전에 사용하던 데이터가 남아있을 수 있으므로 초기화하고 써야 한다.

```go
// 위험
lookup := pool.Get().(*TxLookup)
// lookup에 이전 트랜잭션 데이터가 남아있을 수 있음

// 안전
lookup := pool.Get().(*TxLookup)
lookup.hash = Hash{}    // 초기화
lookup.index = 0
```

포인터 타입만 넣는 게 좋다. int 같은 값 타입을 넣으면 interface{}로 박싱/언박싱되면서 결국 할당이 발생한다.

---

## 정리

sync.Pool이 빠른 이유를 한 줄로 요약하면 이렇다.

P마다 로컬 풀을 두고, P는 한 번에 하나의 고루틴만 실행하니까, private 접근 시 락 경쟁이 발생하지 않는다.

이걸 이해하려면 Go 런타임의 GMP 모델을 알아야 했다. 고루틴이 경량인 이유, P가 왜 필요한지, 2단계 스케줄링이 뭔지 알고 나니까 sync.Pool 동작 방식이 자연스럽게 이해됐다.

블록체인 코어처럼 초당 수만 번 객체를 생성/해제하는 시스템에서 이런 이해가 실제 최적화로 이어진다. Geth 코드에서 sync.Pool이 왜 그렇게 많이 쓰이는지, 왜 그 위치에서 쓰이는지 이제 납득이 간다.

결국 Go의 동시성 모델 전체가 유기적으로 연결되어 있다. 하나를 깊이 파면 다른 것들도 따라온다.

---

## 참고

- Go 런타임 소스 코드: runtime/proc.go, sync/pool.go
- Go Memory Model: https://go.dev/ref/mem
- GMP 스케줄러 설계 문서: https://go.dev/src/runtime/HACKING
- Geth 소스: https://github.com/ethereum/go-ethereum
- CometBFT 소스: https://github.com/cometbft/cometbft
