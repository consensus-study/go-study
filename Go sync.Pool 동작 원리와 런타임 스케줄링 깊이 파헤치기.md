Go로 고성능 네트워크 프로그램을 작성하다 보면 sync.Pool을 자주 마주치게 된다. 대부분의 글에서는 "객체를 재사용해서 GC 부담을 줄인다" 정도로 설명하고 넘어가는데, 실제로 내부에서 어떻게 동작하는지 궁금해서 파보기 시작했다.

결론부터 말하면, sync.Pool을 제대로 이해하려면 Go 런타임의 스케줄링 구조(GMP 모델)를 먼저 알아야 한다. 이 글에서는 그 과정을 정리해본다.

---

## 전통적인 멀티스레드 모델의 한계

일반적인 멀티스레드 프로그래밍에서는 OS 스레드가 CPU 코어를 두고 경쟁한다.

```
스레드 T1, T2, T3, T4, T5 ...
        |
        v
  OS 스케줄러 (커널)
        |
        v
  CPU 0   CPU 1   CPU 2   CPU 3
```

문제는 스레드 간 전환(Context Switching)이 발생할 때마다 커널이 개입한다는 점이다. 레지스터 저장, 복원, TLB 플러시 등의 작업이 필요하고, 이 비용이 1~10 마이크로초 정도 든다. 스레드가 수백 개만 되어도 스위칭 오버헤드가 무시할 수 없는 수준이 된다.

그리고 스레드 하나당 스택 메모리가 보통 1~8MB 필요하다. 연결 10,000개를 동시에 처리하려고 스레드 10,000개를 만들면 그것만으로 수십 GB 메모리가 필요해진다.

---

## Go의 해결책: GMP 모델

Go는 이 문제를 해결하기 위해 자체 스케줄링 시스템을 도입했다. 흔히 GMP 모델이라고 부른다.

- G (Goroutine): 고루틴. 우리가 `go func()`으로 만드는 것
- M (Machine): OS 스레드. 실제로 CPU에서 실행되는 단위
- P (Processor): 논리적 프로세서. 고루틴을 M에 할당하는 중간 계층

```
고루틴 G1, G2, G3 ... G10000
        |
        v
  Go 런타임 스케줄러 (유저 공간)
        |
        v
    P0      P1      P2      P3      (GOMAXPROCS 개수만큼)
    |       |       |       |
    v       v       v       v
    M0      M1      M2      M3      (OS 스레드)
    |       |       |       |
    v       v       v       v
  CPU 0   CPU 1   CPU 2   CPU 3
```

핵심은 2단계 스케줄링이다.

1단계에서 고루틴들이 P를 두고 경쟁한다. 이 과정은 Go 런타임이 유저 공간에서 처리한다. 커널 개입이 없어서 빠르다.

2단계에서 M(OS 스레드)이 CPU를 두고 경쟁한다. 이건 OS 커널이 관리한다.

비유하자면, 직원 10,000명이 사장(OS)한테 직접 보고하는 게 아니라, 팀장 4명(P)한테 먼저 배정되고, 팀장이 사장한테 보고하는 구조다. 대부분의 스케줄링 결정을 팀장 선에서 처리하니까 사장(커널)이 개입할 일이 줄어든다.

---

## 숫자로 보는 차이

| 항목 | OS 스레드 | 고루틴 |
|------|-----------|--------|
| 스택 메모리 | 1~8MB (고정) | 2KB (동적 증가) |
| 생성 비용 | ~1ms | ~0.3μs |
| 스위칭 비용 | 1~10μs (커널 개입) | 0.1~0.2μs (유저 공간) |
| 생성 가능 개수 | 수천 개 | 수십만 개 |

고루틴 10,000개를 만들어도 메모리는 약 20MB밖에 안 든다. OS 스레드로 했으면 10GB다.

---

## P의 개수는 고정이다

여기서 중요한 점이 있다. P의 개수는 프로그램 시작 시 GOMAXPROCS 값으로 고정된다. 보통 CPU 코어 수와 같다. 8코어 CPU라면 P가 8개다.

고루틴을 아무리 많이 만들어도 P가 늘어나지 않는다.

```
CPU 8코어 -> GOMAXPROCS = 8 -> P 8개 (고정)

고루틴 10,000개 만들어도 P는 여전히 8개
동시에 실행되는 고루틴도 최대 8개
```

나머지 9,992개의 고루틴은 P의 로컬 런큐에서 대기하거나 글로벌 런큐에 들어간다. 실행 중인 고루틴이 I/O 대기 상태가 되면 잠깐 빠지고, 대기 중이던 다른 고루틴이 실행된다.

---

## 다시 sync.Pool로 돌아와서

이제 sync.Pool 이야기를 할 수 있다.

sync.Pool은 P마다 로컬 풀을 가지고 있다.

```
P0              P1              P2              P3
[private]       [private]       [private]       [private]
[shared]        [shared]        [shared]        [shared]
```

각 P의 풀에는 두 가지 저장소가 있다.

- private: 해당 P 전용. 딱 1개만 저장. 락 없이 접근.
- shared: 다른 P도 접근 가능. 여러 개 저장. 락 필요.

---

## Get() 호출 시 일어나는 일

pool.Get()을 호출하면 다음 순서로 버퍼를 찾는다.

1. 현재 P의 private 확인 -> 있으면 반환 (락 없음)
2. 현재 P의 shared 확인 -> 있으면 반환
3. 다른 P의 shared에서 훔쳐오기 (work stealing)
4. 전부 없으면 New() 호출해서 새로 생성

---

## 왜 private 접근에 락이 필요 없는가

처음에 이게 이해가 안 됐다. 고루틴이 10,000개인데 어떻게 락 없이 동시 접근을 막는다는 건가.

답은 간단했다. 동시에 접근할 일이 없다.

P는 한 번에 하나의 고루틴만 실행한다. P0에서 G1이 실행 중이면, G5, G100, G9999 같은 다른 고루틴들은 전부 대기 상태다. 대기 중인 고루틴은 코드 실행 자체를 못 한다. pool.Get()을 호출할 수가 없다.

```
P0에서 G1 실행 중
    |
    v
pool.Get() 호출
    |
    v
P0의 private 접근
    |
    v
다른 고루틴이 동시에 접근 가능? 불가능.
P0은 지금 G1만 실행 중이니까.
```

그래서 private은 락이 필요 없다. 경쟁 상대가 없다.

---

## 풀이 P 개수만큼만 있어도 충분한 이유

고루틴이 10,000개여도 동시에 버퍼를 요청할 수 있는 건 P 개수만큼이다. 8코어면 최대 8개.

나머지 9,992개는 대기 중이라 버퍼를 요청할 일 자체가 없다. 그래서 풀이 8개면 고루틴 10,000개를 커버할 수 있다.

만약 고루틴마다 풀을 따로 가지고 있다면 대부분 대기 중인데 쓸데없이 메모리를 잡아먹고 있는 셈이 된다.

---

## Work Stealing

P0의 풀이 비어있는데 P2의 풀에는 버퍼가 남아있는 상황이 있을 수 있다. 이때 P0은 P2의 shared에서 버퍼를 훔쳐온다.

이 방식으로 바쁜 P는 한가한 P에서 자원을 가져오고, 전체적으로 자원이 재분배된다. New() 호출을 줄이고, GC 부담도 줄어든다.

---

## RWMutex의 RLock과 다른 점

RLock도 읽기 락이라 여러 고루틴이 동시에 잡을 수 있다. 하지만 여전히 락을 잡는 행위 자체가 있다. atomic 연산으로 읽기 카운터를 증가시키고, 쓰기 대기자가 있는지 확인하는 로직이 들어간다.

sync.Pool의 private 접근은 이런 게 아예 없다. 그냥 값을 가져온다. P가 한 번에 하나의 고루틴만 실행하니까 경쟁 자체가 발생하지 않는다.

| 방식 | 락 연산 | atomic 연산 | 경쟁 가능성 |
|------|---------|-------------|-------------|
| RLock | O (가벼운 락) | O | O |
| sync.Pool private | X | X | X |

---

## 주의사항

sync.Pool은 캐시일 뿐 영구 저장소가 아니다. GC가 발생하면 풀의 내용이 사라질 수 있다.

```go
pool.Put(buf)
runtime.GC()
buf = pool.Get() // New()가 다시 호출될 수 있음
```

그리고 풀에서 꺼낸 객체는 이전에 사용하던 데이터가 남아있을 수 있으므로 초기화하고 써야 한다.

---

## 정리

sync.Pool이 빠른 이유를 한 줄로 요약하면 이렇다.

P마다 로컬 풀을 두고, P는 한 번에 하나의 고루틴만 실행하니까, private 접근 시 락 경쟁이 발생하지 않는다.

이걸 이해하려면 Go 런타임의 GMP 모델을 알아야 했다. 고루틴이 경량인 이유, P가 왜 필요한지, 2단계 스케줄링이 뭔지 알고 나니까 sync.Pool 동작 방식이 자연스럽게 이해됐다.

결국 Go의 동시성 모델 전체가 유기적으로 연결되어 있다. 하나를 깊이 파면 다른 것들도 따라온다.

---

## 참고

- Go 런타임 소스 코드: runtime/proc.go, sync/pool.go
- Go Memory Model: https://go.dev/ref/mem
- GMP 스케줄러 설계 문서: https://go.dev/src/runtime/HACKING
